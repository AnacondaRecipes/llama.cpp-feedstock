{% set name = "llama.cpp" %}
{% set version = "0.0.2408" %}
{% set build_number = 0 %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://github.com/ggerganov/llama.cpp/archive/b{{ version.split(".")[-1] }}.tar.gz
  sha256: 163255cb77599be6f97c487bcdce79912c7509a9c522ad4ab5b23cc4104d0bb4
  patches:                        # [linux or (unix and blas_impl == "mkl")]
    - test_exe_threads.patch      # [linux]
    - mkl.patch                   # [blas_impl == "mkl"]

build:
  # skip_gpu is set through abs.yaml for use in prefect only
  skip: true # [skip_gpu and ((gpu_variant == "cuda-11") or (gpu_variant == "cuda-12"))]
  # Use a build number difference to ensure that the GPU
  # variant is slightly preferred by conda's solver, so that it's preferentially
  # installed where the platform supports it.
  number: {{ build_number + 110 }}  # [gpu_variant == "cuda-12"]
  number: {{ build_number + 100 }}  # [(gpu_variant == "cuda-11") or (gpu_variant == "metal")]
  number: {{ build_number }}        # [gpu_variant == "none"]
  string: cuda{{ cuda_compiler_version | replace('.', '') }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
  string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                 # [gpu_variant == "none"]
  string: mps_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                 # [gpu_variant == "metal"]
  missing_dso_whitelist:                                                                         # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
    - "**/libcuda.so*"                                                                           # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
  
requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}                              # [gpu_variant == "cuda-12"]
    - cmake
    - ninja-base
    - pkgconfig
    - patch     # [linux or (unix and blas_impl == "mkl")]
    - m2-patch  # [win and blas_impl == "mkl"]
    - pthread-stubs 0.3                                   # [linux]
  host:
    - cudatoolkit      {{ cuda_compiler_version }}        # [gpu_variant == "cuda-11"]
    - cuda-version     {{ cuda_compiler_version }}        # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
    - cuda-cudart-dev  {{ cuda_compiler_version }}        # [gpu_variant == "cuda-12"]
    - libcublas-dev    {{ cuda_compiler_version }}        # [gpu_variant == "cuda-12"]

    - openblas-devel {{ openblas }}                       # [blas_impl == "openblas"]
    - mkl-devel {{ mkl }}                                 # [blas_impl == "mkl"]
  run:
    - {{ pin_compatible('cudatoolkit', max_pin='x.x') }}  # [gpu_variant == "cuda-11"]
    - {{ pin_compatible('cuda-version', max_pin='x.x') }} # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]

test:
  commands:
    - main --help
    - server --help

about:
  home: https://github.com/ggerganov/llama.cpp
  summary: LLM inference in C/C++
  description: |
    Inference of Meta's LLaMA model (and others) in pure C/C++
  license: MIT
  license_family: MIT
  license_file: LICENSE
  dev_url: https://github.com/ggerganov/llama.cpp
  doc_url: https://github.com/ggerganov/llama.cpp

extra:
  recipe-maintainers:
    - sodre
