{% set name = "llama.cpp-meta" %}
{% set upstream_release = "b5943" %}
{% set upstream_commit = "a979ca22db0d737af1e548a73291193655c6be99" %}
{% set version = "0.0." + upstream_release[1:] %}
{% set gguf_version = "0.17.1." + upstream_release[1:] %}
{% set build_number = 0 %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://github.com/ggml-org/llama.cpp/archive/{{ upstream_release }}.tar.gz
  sha256: abb05cabbb7f2b18e762f0db17c4842836a768e33c4a8b840a4d3dbf8cdc47d4

  patches:
    - patches/mkl.patch                     # [blas_impl == "mkl"]
    - patches/metal_gpu_selection.patch     # [osx]
    - patches/hwcap_sve_check.patch         # [linux and aarch64]
    - patches/no-armv9-support-gcc11.patch  # [linux and aarch64]
    - patches/fix-convert_lora_to_gguf.patch

build:
  skip: true # [skip_cuda_prefect and (gpu_variant or "").startswith('cuda')]
  number: {{ build_number }}

outputs:
  - name: llama.cpp
    script: build-llama-cpp.sh   # [not win]
    script: bld-llama-cpp.bat   # [win]
    version: {{ version }}

    build:
      script_env:
        - LLAMA_BUILD_NUMBER={{ upstream_release[1:] }}
        - LLAMA_BUILD_COMMIT={{ upstream_commit}}  
      # skip_cuda_prefect is set through abs.yaml for use in prefect only
      skip: true # [skip_cuda_prefect and (gpu_variant or "").startswith('cuda')]
      # do not mix cublas and mkl/openblas
      skip: true # [((gpu_variant or "").startswith('cuda') and (blas_impl != "cublas")) or (not (gpu_variant or "").startswith('cuda') and (blas_impl == "cublas"))]
      # Use a build number difference to ensure that the GPU
      # variant is slightly preferred by conda's solver, so that it's preferentially
      # installed where the platform supports it.
      number: {{ build_number + 100 }}  # [(gpu_variant or "").startswith('cuda')]
      number: {{ build_number }}        # [gpu_variant == "none"
      string: cuda{{ cuda_compiler_version | replace('.', '') }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [(gpu_variant or "").startswith('cuda')]
      string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                              # [gpu_variant == "none"]
      string: mps_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                 # [gpu_variant == "metal"]
      missing_dso_whitelist:                                                                         # [(gpu_variant or "").startswith('cuda')]
        - "**/libcuda.so*"                                                                           # [(gpu_variant or "").startswith('cuda')]
      
    requirements:
      build:
        - {{ stdlib('c')}}
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}                              # [(gpu_variant or "").startswith('cuda')]
        - cmake
        - ninja-base
        - pkgconfig
      host:
        - cuda-version     {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - cuda-cudart-dev  {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - libcublas-dev    {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - cuda-compat      {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda') and linux]
        - openblas-devel {{ openblas }}                       # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "openblas"]
        - mkl-devel {{ mkl }}                                 # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
        - intel-openmp {{ mkl }}                              # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
        - llvm-openmp 17.0.6                                  # [osx]
        - libcurl {{ libcurl }}

      run:
        - {{ pin_compatible('cuda-version', max_pin='x.x') }} # [(gpu_variant or "").startswith('cuda')]
        - {{ pin_compatible('intel-openmp') }}                # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
        - llvm-openmp                                         # [osx] bounds through run_exports
        - _openmp_mutex                                       # [linux]
        - libcurl # bounds through run_exports

    test:
      commands:
        - llama-cli --help
        - llama-server --help
        - test -f $PREFIX/include/llama.h    # [unix]
        - test -f $PREFIX/bin/llama-cli       # [unix]
        - test -f $PREFIX/bin/llama-server   # [unix]
        - test -f $PREFIX/lib/libllama.so    # [linux]
        - test -f $PREFIX/lib/libllama.dylib # [osx]
        - if not exist %PREFIX%/Library/include/llama.h exit 1  # [win]
        - if not exist %PREFIX%/Library/lib/llama.lib exit 1    # [win]
        - if not exist %PREFIX%/Library/bin/llama.dll exit 1    # [win]
        - if not exist %PREFIX%/Library/bin/llama-cli.exe exit 1      # [win]
        - if not exist %PREFIX%/Library/bin/llama-server.exe exit 1   # [win]

    about:
      home: https://github.com/ggml-org/llama.cpp
      summary: LLM inference in C/C++
      description: |
        Inference of Meta's LLaMA model (and others) in pure C/C++
      license: MIT
      license_family: MIT
      license_file: LICENSE
      dev_url: https://github.com/ggml-org/llama.cpp
      doc_url: https://github.com/ggml-org/llama.cpp

    extra:
      recipe-maintainers:
        - sodre
        - cbouss
        - jnoller

  - name: llama.cpp-tools
    script: build-llama-cpp-tools.sh   # [not win]
    script: bld-llama-cpp-tools.bat  # [win]
    version: {{ version }}

    build:
      entry_points:
        - llama-convert-hf-to-gguf = llama_cpp_tools.convert_hf_to_gguf:main
        - llama-convert-llama-ggml-to-gguf = llama_cpp_tools.convert_llama_ggml_to_gguf:main
        - llama-convert-lora-to-gguf = llama_cpp_tools.convert_lora_to_gguf:main
      skip: True # [py<39]
      number: {{ build_number }}

    requirements:
      host:
        - python
        - poetry-core >=1.0.0
        - pip
      run:
        # This is an aggregate of requirements from multiple files in the llama.cpp-tools repo, see:
        # https://github.com/ggml-org/llama.cpp/tree/master/requirements
        - python
        # requirements/requirements-convert_legacy_llama.txt
        - numpy >=1.26.4 # loosen bounds to accept numpy 2, which appears to be compatible
        - sentencepiece >=0.1.98,<=0.2.0
        - transformers >=4.45.1,<5.0.0
        - protobuf >=4.21.0,<5.0.0
        # requirements/requirements-convert_hf_to_gguf.txt
        # requirements/requirements-convert_llama_ggml_to_gguf.txt
        # requirements/requirements-convert_lora_to_gguf.txt
        - pytorch >=2.2.1
        # temporary until pytorch gets constrained to the matching version of libtorch.
        # current pytorch only has gpu variants for linux-64 and osx-arm64.
        - pytorch >=2.2.1=cpu_*  # [gpu_variant == "none" and ((linux and x86_64) or (osx and arm64))]
        - pytorch >=2.2.1=gpu_*  # [gpu_variant != "none" and ((linux and x86_64) or (osx and arm64))]
        - {{ pin_subpackage('gguf', exact=True) }}
      run_constrained:
        # temporary until pytorch gets constrained to the matching version of libtorch.
        - libtorch=*=cpu_* # [gpu_variant == "none" and ((linux and x86_64) or (osx and arm64))]
        - libtorch=*=gpu_* # [gpu_variant != "none" and ((linux and x86_64) or (osx and arm64))]

    test:
      imports:
        - llama_cpp_tools
      commands:
        - llama-convert-hf-to-gguf --help
        - llama-convert-llama-ggml-to-gguf --help
        - llama-convert-lora-to-gguf --help
      requires:
        - pip

    about:
      home: https://github.com/ggml-org/llama.cpp
      summary: Scripts and conversion tools that ship with llama.cpp
      description: |
        Scripts and conversion tools that ship with llama.cpp
      license: MIT
      license_family: MIT
      license_file: LICENSE
      dev_url: https://github.com/ggml-org/llama.cpp
      doc_url: https://github.com/ggml-org/llama.cpp

    extra:
      recipe-maintainers:
        - sodre
        - cbouss
        - jnoller

  - name: gguf
    script: build-gguf.sh   # [not win]
    script: bld-gguf.bat    # [win]
    version: {{ gguf_version }}
    build:
      entry_points:
        - gguf-convert-endian = gguf.scripts.gguf_convert_endian:main
        - gguf-dump = gguf.scripts.gguf_dump:main
        - gguf-set-metadata = gguf.scripts.gguf_set_metadata:main
        - gguf-new-metadata = gguf.scripts.gguf_new_metadata:main
        - gguf-editor-gui = gguf.scripts.gguf_editor_gui:main
      skip: True # [py<39]
      number: {{ build_number }}

    requirements:
      host:
        - python
        - poetry-core >=1.0.0
        - pip
      run:
        - python
        - numpy >=1.17
        - tqdm >=4.27
        - pyyaml >=5.1
        - sentencepiece >=0.1.98,<=0.2.0
      run_constrained:
        - pyside6 >=6.9,<7.0
    
    test:
      source_files:
        - gguf-py/tests
      imports:
        - gguf
      commands:
        - pip check
        - gguf-convert-endian --help
        - gguf-dump --help
        - gguf-set-metadata --help
        - gguf-new-metadata --help
        - pytest -vv gguf-py/tests
      requires:
        - pip
        - pytest
    about:
      home: https://ggml.ai
      summary: Read and write ML models in GGUF for GGML
      description: |
        Read and write ML models in GGUF for GGML
      license: MIT
      license_family: MIT
      license_file: LICENSE
      dev_url: https://github.com/ggml-org/llama.cpp/tree/master/gguf-py
      doc_url: https://github.com/ggml-org/llama.cpp/tree/master/gguf-py
    extra:
      recipe-maintainers:
        - sodre
        - cbouss
        - jnoller

about:
  home: https://github.com/ggml-org/llama.cpp
  license: MIT
  license_family: MIT
  license_file: LICENSE
  dev_url: https://github.com/ggml-org/llama.cpp
  doc_url: https://github.com/ggml-org/llama.cpp
  summary: LLM inference in C/C++ and GGML conversion tools
  description: |
    Inference of Meta's LLaMA model (and others) in pure C/C++ and GGML conversion tools