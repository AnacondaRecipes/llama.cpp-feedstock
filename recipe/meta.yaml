{% set name = "llama.cpp" %}
{% set version = "0.0.2687" %}
{% set build_number = 0 %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://github.com/ggerganov/llama.cpp/archive/b{{ version.split(".")[-1] }}.tar.gz
  sha256: 02be686a82e267e10e869b4ae8fc37db51abde63e323d19d9ebf4ba33523f5f9
  patches:
    - patches/test_exe_threads.patch      # [linux]
    - patches/mkl.patch                   # [blas_impl == "mkl"]
    - patches/metal_gpu_selection.patch   # [osx]

build:
  # skip_gpu is set through abs.yaml for use in prefect only
  skip: true # [skip_gpu and ((gpu_variant == "cuda-11") or (gpu_variant == "cuda-12"))]
  skip: true # [s390x]
  # Use a build number difference to ensure that the GPU
  # variant is slightly preferred by conda's solver, so that it's preferentially
  # installed where the platform supports it.
  number: {{ build_number + 200 }}  # [gpu_variant == "cuda-12"]
  number: {{ build_number + 100 }}  # [(gpu_variant == "cuda-11") or (gpu_variant == "metal")]
  number: {{ build_number }}        # [gpu_variant == "none"]
  string: cuda{{ cuda_compiler_version | replace('.', '') }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
  string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                 # [gpu_variant == "none"]
  string: mps_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                 # [gpu_variant == "metal"]
  missing_dso_whitelist:                                                                         # [s390x or (gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
    - "**/libcuda.so*"                                                                           # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
    - '$RPATH/ld64.so.1'  # [s390x]
  
requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}                              # [gpu_variant == "cuda-12"]
    - cmake
    - ninja-base
    - pkgconfig
    - patch     # [unix]
    - m2-patch  # [win]
  host:
    - cudatoolkit      {{ cuda_compiler_version }}        # [gpu_variant == "cuda-11"]
    - cuda-version     {{ cuda_compiler_version }}        # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
    - cuda-cudart-dev  {{ cuda_compiler_version }}        # [gpu_variant == "cuda-12"]
    - libcublas-dev    {{ cuda_compiler_version }}        # [gpu_variant == "cuda-12"]
    - openblas-devel {{ openblas }}                       # [blas_impl == "openblas"]
    - mkl-devel {{ mkl }}                                 # [blas_impl == "mkl"]
    - pthread-stubs 0.3                                   # [linux]
  run:
    - {{ pin_compatible('cudatoolkit', max_pin='x.x') }}  # [gpu_variant == "cuda-11"]
    - {{ pin_compatible('cuda-version', max_pin='x.x') }} # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]

test:
  commands:
    - main --help
    - server --help

about:
  home: https://github.com/ggerganov/llama.cpp
  summary: LLM inference in C/C++
  description: |
    Inference of Meta's LLaMA model (and others) in pure C/C++
  license: MIT
  license_family: MIT
  license_file: LICENSE
  dev_url: https://github.com/ggerganov/llama.cpp
  doc_url: https://github.com/ggerganov/llama.cpp

extra:
  recipe-maintainers:
    - sodre
    - cbouss
