{% set name = "llama.cpp" %}
{% set version = "0.0.2408" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://github.com/ggerganov/llama.cpp/archive/b{{ version.split(".")[-1] }}.tar.gz
  sha256: 163255cb77599be6f97c487bcdce79912c7509a9c522ad4ab5b23cc4104d0bb4
  patches:
    - osx-64-pick-discrete.patch  # [osx]
    - mkl.patch                   # [blas_impl == "mkl"]

build:
  number: 0
  skip: true
  string: cuda{{ cuda_compiler_version | replace('.', '') }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
  string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                 # [gpu_variant == "none"]
  string: mps_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                 # [gpu_variant == "metal"]
  missing_dso_whitelist:                                                                         # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
    - "**/libcuda.so*"                                                                           # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
  
# TODO implement cpu/gpu variant selection pattern from pytorch recipe
requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}                              # [gpu_variant == "cuda-12"]
    - cmake
    - git
    - ninja
    - pkgconfig
  host:
    - cudatoolkit      {{ cuda_compiler_version }}        # [gpu_variant == "cuda-11"]
    - cuda-version     {{ cuda_compiler_version }}        # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
    - cuda-cudart-dev  {{ cuda_compiler_version }}        # [gpu_variant == "cuda-12"]
    - libcublas-dev    {{ cuda_compiler_version }}        # [gpu_variant == "cuda-12"]

    - blas-devel * *{{ blas_impl }}                       # [blas_impl == "mkl"]
    - mkl-devel {{ mkl }}                                 # [blas_impl == "mkl"]
  run:
    - {{ pin_compatible('cudatoolkit', max_pin='x.x') }}  # [gpu_variant == "cuda-11"]
    - {{ pin_compatible('cuda-version', max_pin='x.x') }} # [(gpu_variant == "cuda-11") or (gpu_variant == "cuda-12")]
    - llvm-openmp                                         # [linux and blas_impl == "mkl"]

test:
  commands:
    - main --help
    - server --help

about:
  home: https://github.com/ggerganov/llama.cpp
  summary: Port of Facebook's LLaMA model in C/C++
  license: MIT
  license_family: MIT
  license_file: LICENSE

extra:
  recipe-maintainers:
    - sodre
