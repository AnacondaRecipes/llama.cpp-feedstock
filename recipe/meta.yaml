{% set name = "llama.cpp" %}
{% set version = "0.0.3604" %}
{% set build_number = 0 %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://github.com/ggerganov/llama.cpp/archive/b{{ version.split(".")[-1] }}.tar.gz
  sha256: cabd2074b1132dc89ec60b166a3b679d5ec747d50487f8b0dbf76497817ac4f5
  patches:
    - patches/mkl.patch                   # [blas_impl == "mkl"]
    - patches/metal_gpu_selection.patch   # [osx]
    - patches/loosen-max_nmse_err.patch   # [osx]

build:
  # skip_cuda_prefect is set through abs.yaml for use in prefect only
  skip: true # [skip_cuda_prefect and (gpu_variant or "").startswith('cuda')]
  skip: true # [s390x]
  skip: true # [gpu_variant == "cuda-11"]
  # do not mix cublas and mkl/openblas
  skip: true # [((gpu_variant or "").startswith('cuda') and (blas_impl != "cublas")) or (not (gpu_variant or "").startswith('cuda') and (blas_impl == "cublas"))]
  # Use a build number difference to ensure that the GPU
  # variant is slightly preferred by conda's solver, so that it's preferentially
  # installed where the platform supports it.
  number: {{ build_number + 250 }}  # [(gpu_variant or "").startswith('cuda') and (gpu_variant != "cuda-11") and (x86_64_opt == "v3")]
  number: {{ build_number + 200 }}  # [(gpu_variant or "").startswith('cuda') and (gpu_variant != "cuda-11") and (x86_64_opt == "none")]
  number: {{ build_number + 150 }}  # [((gpu_variant == "cuda-11") and (x86_64_opt == "v3"))]
  number: {{ build_number + 100 }}  # [((gpu_variant == "cuda-11") and (x86_64_opt == "none")) or (gpu_variant == "metal")]
  number: {{ build_number + 50 }}   # [gpu_variant == "none" and x86_64_opt == "v3"]
  number: {{ build_number }}        # [gpu_variant == "none" and x86_64_opt == "none"]
  string: cuda{{ cuda_compiler_version | replace('.', '') }}_v3_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [(gpu_variant or "").startswith('cuda') and (x86_64_opt == "v3")]
  string: cuda{{ cuda_compiler_version | replace('.', '') }}_v1_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [(gpu_variant or "").startswith('cuda') and (x86_64_opt == "none")]
  string: cpu_v3_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                              # [gpu_variant == "none" and x86_64_opt == "v3"]
  string: cpu_v1_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                              # [gpu_variant == "none" and x86_64_opt == "none"]
  string: mps_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                 # [gpu_variant == "metal"]
  missing_dso_whitelist:                                                                         # [s390x or (gpu_variant or "").startswith('cuda')]
    - "**/libcuda.so*"                                                                           # [(gpu_variant or "").startswith('cuda')]
    - '$RPATH/ld64.so.1'  # [s390x]
  
requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}                              # [(gpu_variant or "").startswith('cuda') and (gpu_variant != "cuda-11")]
    - cmake
    - ninja-base
    - pkgconfig
    - patch     # [unix]
    - m2-patch  # [win]
  host:
    - cudatoolkit      {{ cuda_compiler_version }}        # [gpu_variant == "cuda-11"]
    - cuda-version     {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
    - cuda-cudart-dev  {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda') and (gpu_variant != "cuda-11")]
    - libcublas-dev    {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda') and (gpu_variant != "cuda-11")]
    - cuda-compat      {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda') and (gpu_variant != "cuda-11") and linux]
    - openblas-devel {{ openblas }}                       # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "openblas"]
    - mkl-devel {{ mkl }}                                 # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
    - intel-openmp {{ mkl }}                              # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
    - llvm-openmp 14.0.6                                  # [osx]
  run:
    - {{ pin_compatible('cudatoolkit', max_pin='x.x') }}  # [gpu_variant == "cuda-11"]
    - {{ pin_compatible('cuda-version', max_pin='x.x') }} # [(gpu_variant or "").startswith('cuda')]
    - {{ pin_compatible('intel-openmp') }}                # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
    - llvm-openmp                                         # [osx] bounds through run_exports
    - _openmp_mutex                                       # [linux]
    - __osx >={{ MACOSX_DEPLOYMENT_TARGET|default("10.12") }}  # [osx and x86_64]
    - _x86_64-microarch-level >=3                         # [x86_64_opt == "v3"]

test:
  commands:
    # clients now return 1 for after printing help...
    # see https://github.com/ggerganov/llama.cpp/pull/7675
    # see https://github.com/ggerganov/llama.cpp/blob/b3131/examples/main/main.cpp#L120-L127
    - llama-cli --help   || true               # [not win]
    - llama-cli --help   || cmd /c "exit /B 0" # [win]
    - llama-server --help || true               # [not win]
    - llama-server --help || cmd /c "exit /B 0" # [win]
    - test -f $PREFIX/include/llama.h    # [unix]
    - test -f $PREFIX/bin/llama-cli       # [unix]
    - test -f $PREFIX/bin/llama-server   # [unix]
    - test -f $PREFIX/lib/libllama.so    # [linux]
    - test -f $PREFIX/lib/libllama.dylib # [osx]
    - if not exist %PREFIX%/Library/include/llama.h exit 1  # [win]
    - if not exist %PREFIX%/Library/lib/llama.lib exit 1    # [win]
    - if not exist %PREFIX%/Library/bin/llama.dll exit 1    # [win]
    - if not exist %PREFIX%/Library/bin/llama-cli.exe exit 1      # [win]
    - if not exist %PREFIX%/Library/bin/llama-server.exe exit 1   # [win]

about:
  home: https://github.com/ggerganov/llama.cpp
  summary: LLM inference in C/C++
  description: |
    Inference of Meta's LLaMA model (and others) in pure C/C++
  license: MIT
  license_family: MIT
  license_file: LICENSE
  dev_url: https://github.com/ggerganov/llama.cpp
  doc_url: https://github.com/ggerganov/llama.cpp

extra:
  recipe-maintainers:
    - sodre
    - cbouss
