{% set name = "llama.cpp-meta" %}
{% set upstream_release = "b7710" %}
{% set upstream_commit = "2bbe4c2cf8298114e3908e285125b9d0d1c5bb42" %}
{% set version = "0.0." + upstream_release[1:] %}
{% set gguf_version = "0.17.1." + upstream_release[1:] %}
{% set build_number = 0 %}

# When output_set is llama_cpp_tools, PBP trips on undefined variables
# because they are not part of the variant config.
# So we set them to 999.0a0 to avoid the render error.
# Setting to 999.0a0 is safe because if they ever get used in the build, they
# will generate a solve error.
{% if output_set == "llama_cpp_tools" %}
{% set mkl = "999.0a0" %}
{% set openblas = "999.0a0" %}
{% set cuda_compiler_version = "999.0a0" %}
{% endif %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://github.com/ggml-org/llama.cpp/archive/{{ upstream_release }}.tar.gz
  sha256: 4c5d3d38221a27ce4a75e406d8e5229fab8b0004f579365167f8b900e0f35181

  patches:
    - patches/fix-macos-dylib-version.patch
    - patches/increase-nmse-tolerance.patch
    - patches/increase-nmse-tolerance-aarch64.patch  # [linux and aarch64]
    - patches/mkl.patch                     # [blas_impl == "mkl"]
    - patches/metal_gpu_selection.patch     # [osx]
    - patches/disable-metal-bf16.patch      # [osx]
    - patches/disable-metal-flash-attention.patch  # [osx]
    - patches/fix-convert_lora_to_gguf.patch
    - patches/fix-models-path.patch

build:
  number: {{ build_number }}

outputs:
  - name: libllama
    script: build-llama-cpp.sh   # [not win]
    script: bld-llama-cpp.bat   # [win]
    version: {{ version }}

    build:
      script_env:
        - LLAMA_BUILD_NUMBER={{ upstream_release[1:] }}
        - LLAMA_BUILD_COMMIT={{ upstream_commit}}
      skip: true # [output_set != "llama"]
      # do not mix cublas and mkl/openblas
      skip: true # [((gpu_variant or "").startswith('cuda') and (blas_impl != "cublas")) or (not (gpu_variant or "").startswith('cuda') and (blas_impl == "cublas"))]
      # Use a build number difference to ensure that the GPU
      # variant is slightly preferred by conda's solver, so that it's preferentially
      # installed where the platform supports it.
      # Different CUDA versions get different build numbers so the latest variant is preferred
      number: {{ build_number + 110 }}  # [(gpu_variant or "").startswith('cuda') and cuda_compiler_version == "13.0"]
      number: {{ build_number + 100 }}  # [(gpu_variant or "").startswith('cuda') and cuda_compiler_version == "12.8"]
      number: {{ build_number + 100 }}  # [gpu_variant == "metal"]
      number: {{ build_number }}        # [gpu_variant == "none"]
      string: cuda{{ cuda_compiler_version | replace('.', '') }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [(gpu_variant or "").startswith('cuda')]
      string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                 # [gpu_variant == "none"]
      string: mps_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                 # [gpu_variant == "metal"]
      missing_dso_whitelist:                                                                         # [(gpu_variant or "").startswith('cuda')]
        - "**/libcuda.so*"  # [(gpu_variant or "").startswith('cuda')]
        - "**/nvcuda.dll"   # [(gpu_variant or "").startswith('cuda')]
      run_exports:
        - {{ pin_subpackage('libllama', exact=True) }}

    requirements:
      build:
        - {{ stdlib('c')}}
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}                              # [(gpu_variant or "").startswith('cuda')]
        - cmake
        - ninja-base
        - pkgconfig
      host:
        - cuda-version     {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - cuda-cudart-dev  {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - libcublas-dev    {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - cuda-compat      {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda') and linux]
        - openblas-devel {{ openblas }}                       # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "openblas"]
        - mkl-devel {{ mkl }}                                 # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
        - intel-openmp {{ mkl }}                              # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
        - llvm-openmp 17.0.6                                  # [osx]
        - libcurl {{ libcurl }}
      run:
        - {{ pin_compatible('cuda-version', max_pin='x.x') }} # [(gpu_variant or "").startswith('cuda')]
        - {{ pin_compatible('intel-openmp') }}                # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
        - llvm-openmp                                         # [osx] bounds through run_exports
        - _openmp_mutex                                       # [linux]
        - cuda-version     {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - __cuda                                              # [(gpu_variant or "").startswith('cuda')]
        - libcurl # bounds through run_exports

    test:
      commands:
        - test -f $PREFIX/include/llama.h    # [unix]
        - test -f $PREFIX/lib/libllama.so    # [linux]
        - test -f $PREFIX/lib/libllama.dylib # [osx]
        - if not exist %PREFIX%/Library/include/llama.h exit 1  # [win]
        - if not exist %PREFIX%/Library/lib/llama.lib exit 1    # [win]
        - if not exist %PREFIX%/Library/bin/llama.dll exit 1    # [win]

    about:
      home: https://github.com/ggml-org/llama.cpp
      summary: LLM inference in C/C++
      description: |
        Inference of Meta's LLaMA model (and others) in pure C/C++
      license: MIT
      license_family: MIT
      license_file: LICENSE
      dev_url: https://github.com/ggml-org/llama.cpp
      doc_url: https://github.com/ggml-org/llama.cpp

  - name: llama.cpp
    script: build-llama-cpp.sh   # [not win]
    script: bld-llama-cpp.bat   # [win]
    version: {{ version }}

    build:
      script_env:
        - LLAMA_BUILD_NUMBER={{ upstream_release[1:] }}
        - LLAMA_BUILD_COMMIT={{ upstream_commit}}
      skip: true # [output_set != "llama"]
      # do not mix cublas and mkl/openblas
      skip: true # [((gpu_variant or "").startswith('cuda') and (blas_impl != "cublas")) or (not (gpu_variant or "").startswith('cuda') and (blas_impl == "cublas"))]
      # Use a build number difference to ensure that the GPU
      # variant is slightly preferred by conda's solver, so that it's preferentially
      # installed where the platform supports it.
      # Different CUDA versions get different build numbers so the latest variant is preferred
      number: {{ build_number + 110 }}  # [(gpu_variant or "").startswith('cuda') and cuda_compiler_version == "13.0"]
      number: {{ build_number + 100 }}  # [(gpu_variant or "").startswith('cuda') and cuda_compiler_version == "12.8"]
      number: {{ build_number + 100 }}  # [gpu_variant == "metal"]
      number: {{ build_number }}        # [gpu_variant == "none"]
      string: cuda{{ cuda_compiler_version | replace('.', '') }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [(gpu_variant or "").startswith('cuda')]
      string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                 # [gpu_variant == "none"]
      string: mps_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                 # [gpu_variant == "metal"]
      missing_dso_whitelist:                                                                         # [(gpu_variant or "").startswith('cuda')]
        - "**/libcuda.so*"  # [(gpu_variant or "").startswith('cuda')]
        - "**/nvcuda.dll"   # [(gpu_variant or "").startswith('cuda')]
      run_exports:
        - {{ pin_subpackage('llama.cpp', exact=True) }}
      
    requirements:
      build:
        - {{ stdlib('c')}}
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}                              # [(gpu_variant or "").startswith('cuda')]
        - cmake
        - ninja-base
        - pkgconfig
      host:
        - cuda-version     {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - cuda-cudart-dev  {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - libcublas-dev    {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - cuda-compat      {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda') and linux]
        - openblas-devel {{ openblas }}                       # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "openblas"]
        - mkl-devel {{ mkl }}                                 # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
        - intel-openmp {{ mkl }}                              # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
        - llvm-openmp 17.0.6                                  # [osx]
        - libcurl {{ libcurl }}
        - {{ pin_subpackage('libllama', exact=True) }}
      run:
        - {{ pin_subpackage('libllama', exact=True) }}
        - {{ pin_compatible('cuda-version', max_pin='x.x') }} # [(gpu_variant or "").startswith('cuda')]
        - {{ pin_compatible('intel-openmp') }}                # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
        - llvm-openmp                                         # [osx] bounds through run_exports
        - _openmp_mutex                                       # [linux]
        - cuda-version     {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - __cuda                                              # [(gpu_variant or "").startswith('cuda')]
        - libcurl # bounds through run_exports

    test:
      commands:
        - llama-cli --version
        - llama-server --version
        - test -f $PREFIX/bin/llama-cli      # [unix]
        - test -f $PREFIX/bin/llama-server   # [unix]
        - if not exist %PREFIX%/Library/bin/llama-cli.exe exit 1      # [win]
        - if not exist %PREFIX%/Library/bin/llama-server.exe exit 1   # [win]

    about:
      home: https://github.com/ggml-org/llama.cpp
      summary: llama.cpp server example and tools, using libllama
      description: |
        Server example and tools for llama.cpp, using libllama
      license: MIT
      license_family: MIT
      license_file: LICENSE
      dev_url: https://github.com/ggml-org/llama.cpp
      doc_url: https://github.com/ggml-org/llama.cpp

  - name: llama.cpp-tests
    script: build-llama-cpp.sh   # [not win]
    script: bld-llama-cpp.bat   # [win]
    version: {{ version }}

    build:
      script_env:
        - LLAMA_BUILD_NUMBER={{ upstream_release[1:] }}
        - LLAMA_BUILD_COMMIT={{ upstream_commit}}
      skip: true # [output_set != "llama"] 
      # do not mix cublas and mkl/openblas
      skip: true # [((gpu_variant or "").startswith('cuda') and (blas_impl != "cublas")) or (not (gpu_variant or "").startswith('cuda') and (blas_impl == "cublas"))]
      # Use a build number difference to ensure that the GPU
      # variant is slightly preferred by conda's solver, so that it's preferentially
      # installed where the platform supports it.
      # Different CUDA versions get different build numbers so the latest variant is preferred
      number: {{ build_number + 110 }}  # [(gpu_variant or "").startswith('cuda') and cuda_compiler_version == "13.0"]
      number: {{ build_number + 100 }}  # [(gpu_variant or "").startswith('cuda') and cuda_compiler_version == "12.8"]
      number: {{ build_number + 100 }}  # [gpu_variant == "metal"]
      number: {{ build_number }}        # [gpu_variant == "none"]
      string: cuda{{ cuda_compiler_version | replace('.', '') }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [(gpu_variant or "").startswith('cuda')]
      string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                 # [gpu_variant == "none"]
      string: mps_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                 # [gpu_variant == "metal"]
      missing_dso_whitelist:                                                                         # [(gpu_variant or "").startswith('cuda')]
        - "**/libcuda.so*"  # [(gpu_variant or "").startswith('cuda')]
        - "**/nvcuda.dll"   # [(gpu_variant or "").startswith('cuda')]

    requirements:
      build:
        - {{ stdlib('c')}}
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}                              # [(gpu_variant or "").startswith('cuda')]
        - cmake
        - ninja-base
        - pkgconfig
      host:
        - cuda-version     {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - cuda-cudart-dev  {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - libcublas-dev    {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - cuda-compat      {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda') and linux]
        - openblas-devel {{ openblas }}                       # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "openblas"]
        - mkl-devel {{ mkl }}                                 # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
        - intel-openmp {{ mkl }}                              # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
        - libcurl {{ libcurl }}
        - {{ pin_subpackage('libllama', exact=True) }}
        - {{ pin_subpackage('llama.cpp', exact=True) }}
      run:
        - {{ pin_subpackage('libllama', exact=True) }}
        - {{ pin_subpackage('llama.cpp', exact=True) }}
        - {{ pin_compatible('cuda-version', max_pin='x.x') }} # [(gpu_variant or "").startswith('cuda')]
        - {{ pin_compatible('intel-openmp') }}                # [(not (gpu_variant or "").startswith('cuda')) and blas_impl == "mkl"]
        - _openmp_mutex                                       # [linux]
        - cuda-version     {{ cuda_compiler_version }}        # [(gpu_variant or "").startswith('cuda')]
        - __cuda                                              # [(gpu_variant or "").startswith('cuda')]
        - libcurl # bounds through run_exports

    test:
      commands:
        - test -f $PREFIX/bin/test-tokenizer-0                          # [unix]
        - if not exist %PREFIX%/Library/bin/test-tokenizer-0.exe exit 1 # [win]

    about:
      home: https://github.com/ggml-org/llama.cpp
      summary: llama.cpp test executables
      description: |
        Test executables for llama.cpp
      license: MIT
      license_family: MIT
      license_file: LICENSE
      dev_url: https://github.com/ggml-org/llama.cpp
      doc_url: https://github.com/ggml-org/llama.cpp

  - name: llama.cpp-tools
    script: build-llama-cpp-tools.sh   # [not win]
    script: bld-llama-cpp-tools.bat  # [win]
    version: {{ version }}

    build:
      entry_points:
        - llama-convert-hf-to-gguf = llama_cpp_tools.convert_hf_to_gguf:main
        - llama-convert-llama-ggml-to-gguf = llama_cpp_tools.convert_llama_ggml_to_gguf:main
        - llama-convert-lora-to-gguf = llama_cpp_tools.convert_lora_to_gguf:main
      skip: True # [py<39]
      skip: true # [output_set != "llama_cpp_tools"]
      number: {{ build_number }}

    requirements:
      host:
        - python
        - poetry-core >=1.0.0
        - pip
      run:
        # This is an aggregate of requirements from multiple files in the llama.cpp-tools repo, see:
        # https://github.com/ggml-org/llama.cpp/tree/master/requirements
        - python
        # requirements/requirements-convert_legacy_llama.txt
        - numpy >=1.26.4 # loosen bounds to accept numpy 2, which appears to be compatible
        - sentencepiece >=0.1.98,<=0.2.0
        - transformers >=4.51.3,<5.0.0
        - protobuf >=4.21.0,<6.0.0 # loosen bounds to accept protobuf 5
        # requirements/requirements-convert_hf_to_gguf.txt
        # requirements/requirements-convert_llama_ggml_to_gguf.txt
        # requirements/requirements-convert_lora_to_gguf.txt
        - mistral-common >=1.8.3
        - pytorch >=2.6.0
        - {{ pin_subpackage('gguf', exact=True) }}

    test:
      imports:
        - llama_cpp_tools
      commands:
        # Skip --help on osx: PyTorch has ABI issue (Symbol not found: __ZN2at3mps14getMPSProfilerEv)
        - llama-convert-hf-to-gguf --help  # [not osx]
        - llama-convert-llama-ggml-to-gguf --help  # [not osx]
        - llama-convert-lora-to-gguf --help  # [not osx]
        - test -d $SP_DIR/llama_cpp_tools/models  # [unix]
        - test -f $SP_DIR/llama_cpp_tools/models/ggml-vocab-llama-bpe.gguf  # [unix]
        - test -d $SP_DIR/llama_cpp_tools/models/templates  # [unix]
        - test -f $SP_DIR/llama_cpp_tools/models/templates/README.md  # [unix]
        - if not exist %SP_DIR%\llama_cpp_tools\models exit 1  # [win]
        - if not exist %SP_DIR%\llama_cpp_tools\models\ggml-vocab-llama-bpe.gguf exit 1  # [win]
        - if not exist %SP_DIR%\llama_cpp_tools\models\templates exit 1  # [win]
        - if not exist %SP_DIR%\llama_cpp_tools\models\templates\README.md exit 1  # [win]
      requires:
        - pip

    about:
      home: https://github.com/ggml-org/llama.cpp
      summary: Scripts and conversion tools that ship with llama.cpp
      description: |
        Scripts and conversion tools that ship with llama.cpp, including model vocabulary files and templates
      license: MIT
      license_family: MIT
      license_file: LICENSE
      dev_url: https://github.com/ggml-org/llama.cpp
      doc_url: https://github.com/ggml-org/llama.cpp

  - name: gguf
    script: build-gguf.sh   # [not win]
    script: bld-gguf.bat    # [win]
    version: {{ gguf_version }}
    build:
      entry_points:
        - gguf-convert-endian = gguf.scripts.gguf_convert_endian:main
        - gguf-dump = gguf.scripts.gguf_dump:main
        - gguf-set-metadata = gguf.scripts.gguf_set_metadata:main
        - gguf-new-metadata = gguf.scripts.gguf_new_metadata:main
        - gguf-editor-gui = gguf.scripts.gguf_editor_gui:main      
      skip: True # [py<39]
      skip: true # [output_set != "llama_cpp_tools"]
      number: {{ build_number }}

    requirements:
      host:
        - python
        - poetry-core >=1.0.0
        - pip
      run:
        - python
        - numpy >=1.17
        - tqdm >=4.27
        - pyyaml >=5.1
        - sentencepiece >=0.1.98,<=0.2.0
      run_constrained:
        - pyside6 >=6.9,<7.0
    
    test:
      source_files:
        - gguf-py/tests
      imports:
        - gguf
      commands:
        - pip check
        - gguf-convert-endian --help
        - gguf-dump --help
        - gguf-set-metadata --help
        - gguf-new-metadata --help
        - pytest -vv gguf-py/tests
      requires:
        - pip
        - pytest
    about:
      home: https://ggml.ai
      summary: Read and write ML models in GGUF for GGML
      description: |
        Read and write ML models in GGUF for GGML
      license: MIT
      license_family: MIT
      license_file: LICENSE
      dev_url: https://github.com/ggml-org/llama.cpp/tree/master/gguf-py
      doc_url: https://github.com/ggml-org/llama.cpp/tree/master/gguf-py

about:
  home: https://github.com/ggml-org/llama.cpp
  license: MIT
  license_family: MIT
  license_file: LICENSE
  dev_url: https://github.com/ggml-org/llama.cpp
  doc_url: https://github.com/ggml-org/llama.cpp
  summary: LLM inference in C/C++ and GGML conversion tools
  description: |
    Inference of Meta's LLaMA model (and others) in pure C/C++ and GGML conversion tools

extra:
  recipe-maintainers:
    - sodre
    - cbouss
    - jnoller