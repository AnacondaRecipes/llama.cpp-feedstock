{% set name = "llama.cpp" %}
{% set version = "0.0.2408" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://github.com/ggerganov/llama.cpp/archive/b{{ version.split(".")[-1] }}.tar.gz
  sha256: 163255cb77599be6f97c487bcdce79912c7509a9c522ad4ab5b23cc4104d0bb4
  patches:
    - osx-64-pick-discrete.patch  # [osx]
    - mkl.patch                   # [blas_impl == "mkl"]

build:
  number: 0
  skip: true # [not (linux and x86_64) or (cuda_compiler_version == "None")]
  string: cuda{{ cuda_compiler_version | replace('.', '') }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version != "None"]
  string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                 # [(osx and x86_64) or cuda_compiler_version == "None"]
  string: mps_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                 # [osx and arm64]
  missing_dso_whitelist:
    - "**/libcuda.so*"
 
  script:
    - LLAMA_ARGS="-DLLAMA_BUILD_TESTS=OFF"              # [unix]
    - set LLAMA_ARGS=-DLLAMA_BUILD_TESTS=OFF            # [win]
    {% macro llama_args(value) -%}
    - LLAMA_ARGS="${LLAMA_ARGS} -DLLAMA_{{ value }}"    # [unix]
    - set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_{{ value }}   # [win]
    {%- endmacro %}
    {% macro cmake_args(value) -%}
    - LLAMA_ARGS="${LLAMA_ARGS} -D{{ value }}"          # [unix]
    - set LLAMA_ARGS=%LLAMA_ARGS% -D{{ value }}         # [win]
    {%- endmacro %}

    {{ cmake_args("BUILD_SHARED_LIBS=ON") }}

    {{ llama_args("NATIVE=OFF") }}      # [cpu_variant == "main_channel_default"]
    {{ llama_args("AVX=OFF") }}         # [osx and arm64]
    {{ llama_args("AVX2=OFF") }}        # [osx and arm64]
    {{ llama_args("FMA=OFF") }}         # [osx and arm64]
    {{ llama_args("F16C=OFF") }}        # [osx and arm64]
    {{ llama_args("METAL=ON") }}        # [osx and arm64]
    {{ llama_args("METAL=OFF") }}       # [osx and x86_64]
    {{ llama_args("ACCELERATE=ON") }}   # [osx]

    {{ llama_args("CUBLAS=ON") }}                     # [cuda_compiler_version != "None"]
    {{ cmake_args("CMAKE_CUDA_ARCHITECTURES=all") }}  # [cuda_compiler_version != "None"]

    {{ llama_args("BLAS=ON") }}                     # [not osx and cuda_compiler_version == "None"]
    {{ llama_args("BLAS_VENDOR=Intel10_64_dyn") }}  # [not osx and cuda_compiler_version == "None" and blas_impl == "mkl"]

    - echo $LLAMA_ARGS  # [unix]
    - set LLAMA_ARGS    # [win]

    - export CUDACXX=/usr/local/cuda/bin/nvcc   # [cuda_compiler_version != "None" and not (cuda_compiler_version or "").startswith("12")]
    - export CUDAHOSTCXX="${CXX}"               # [cuda_compiler_version != "None" and not (cuda_compiler_version or "").startswith("12")]
    - export LDFLAGS="$LDFLAGS -Wl,-rpath-link,/usr/lib64"    # [(cuda_compiler_version or "").startswith("12")]

    - cmake -S . -B build -G Ninja %CMAKE_ARGS% %LLAMA_ARGS%    # [win]
    - cmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}  # [unix]
    - cmake --build build
    - cmake --install build 

requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}                        # [(cuda_compiler_version or "").startswith("12")]
    - cmake
    - git
    - ninja
    - pkgconfig
  host:
    - cudatoolkit      {{ cuda_compiler_version }}  # [cuda_compiler_version != "None" and not (cuda_compiler_version or "").startswith("12")]
    - cuda-version     {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
    - cuda-cudart-dev  {{ cuda_compiler_version }}  # [(cuda_compiler_version or "").startswith("12")]
    - libcublas-dev    {{ cuda_compiler_version }}  # [(cuda_compiler_version or "").startswith("12")]

    - blas-devel * *{{ blas_impl }}                 # [not osx and cuda_compiler_version == "None" and blas_impl == "mkl"]
    - mkl-devel {{ mkl }}                           # [not osx and cuda_compiler_version == "None" and blas_impl == "mkl"]
  run:
    - {{ pin_compatible('cudatoolkit', max_pin='x.x') }}  # [cuda_compiler_version != "None" and not (cuda_compiler_version or "").startswith("12")]
    - {{ pin_compatible('cuda-version', max_pin='x.x') }} # [cuda_compiler_version != "None"]

    - llvm-openmp                                   # [linux and cuda_compiler_version == "None" and blas_impl == "mkl"]
    - __cuda >={{ cuda_compiler_version }}          # [cuda_compiler_version != "None" and not (cuda_compiler_version or "").startswith("12")]

test:
  commands:
    - main --help
    - server --help

about:
  home: https://github.com/ggerganov/llama.cpp
  summary: Port of Facebook's LLaMA model in C/C++
  license: MIT
  license_family: MIT
  license_file: LICENSE

extra:
  recipe-maintainers:
    - sodre
